{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "1cTCxGSBB5FsKEpeG5l9PPSnXmD8rWq6z",
      "authorship_tag": "ABX9TyM8OoHlJbRov8Ima5AD5/Mp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Info-stats-ai/Forecasting_Using_XGboost_and_LLM/blob/main/Xgboost%2BLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWB1AJua_b4D",
        "outputId": "47c3d179-5581-43e3-aec3-afa520b1fb1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "✅ Google Drive successfully mounted!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt you for authorization.\n",
        "# Click the link, sign in to your Google account, copy the authorization code,\n",
        "# and paste it back into the box in this cell.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"\\n✅ Google Drive successfully mounted!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries first\n",
        "!pip install pdfplumber -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4zwFLVjhXWE",
        "outputId": "a1bac8d1-589c-40c1-c0c4-6354d690e3c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "jwprxM6-ltsu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "# This path MUST point to the folder containing your UPLOADED ZIP FILES.\n",
        "# Please double-check that this is the correct path.\n",
        "pdf_zip_folder_path = \"/content/drive/My Drive/homeguide_zipped\" # <--- UPDATE THIS PATH if needed\n",
        "# -------------------\n",
        "\n",
        "# These are the temporary folders we will create in the Colab environment.\n",
        "unzipped_pdfs_folder = \"unzipped_pdfs\"\n",
        "text_output_folder = \"extracted_text\"\n",
        "\n",
        "# Create the folders to work in.\n",
        "os.makedirs(unzipped_pdfs_folder, exist_ok=True)\n",
        "os.makedirs(text_output_folder, exist_ok=True)\n",
        "\n",
        "print(\"\\n✅ Drive mounted and folders are set up successfully.\")\n",
        "print(f\"I will look for zip files in: '{pdf_zip_folder_path}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssYMYob8lnEF",
        "outputId": "f0554e55-5466-467f-a221-f860281bb6cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Drive mounted and folders are set up successfully.\n",
            "I will look for zip files in: '/content/drive/My Drive/homeguide_zipped'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"--- Starting Step 3: Unzipping files from '{pdf_zip_folder_path}' ---\")\n",
        "\n",
        "if not os.path.exists(pdf_zip_folder_path):\n",
        "    print(f\"❌ ERROR: The folder '{pdf_zip_folder_path}' was not found.\")\n",
        "else:\n",
        "    zip_files = [f for f in os.listdir(pdf_zip_folder_path) if f.lower().endswith(\".zip\")]\n",
        "    if not zip_files:\n",
        "        print(f\"❌ ERROR: No .zip files were found in '{pdf_zip_folder_path}'.\")\n",
        "    else:\n",
        "        print(f\"Found {len(zip_files)} zip files to process...\")\n",
        "        for zip_filename in zip_files:\n",
        "            print(f\"   Unzipping '{zip_filename}'...\")\n",
        "            zip_path = os.path.join(pdf_zip_folder_path, zip_filename)\n",
        "            try:\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(unzipped_pdfs_folder)\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠️ Could not unzip '{zip_filename}'. Error: {e}\")\n",
        "\n",
        "        # Verify the result by counting the number of items in the output folder.\n",
        "        # Note: This counts files AND any sub-folders that were created.\n",
        "        total_items = len(os.listdir(unzipped_pdfs_folder))\n",
        "        print(f\"\\n✅ Unzipping complete! The '{unzipped_pdfs_folder}' folder now contains {total_items} items.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsD6OLBPlwUN",
        "outputId": "496fd822-e086-430c-ee98-5f73c0c7cf6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Step 3: Unzipping files from '/content/drive/My Drive/homeguide_zipped' ---\n",
            "Found 3 zip files to process...\n",
            "   Unzipping 'Homeguide.zip'...\n",
            "   Unzipping 'Homeguide-20250902T030504Z-1-002.zip'...\n",
            "   Unzipping 'Homeguide-20250902T030504Z-1-001.zip'...\n",
            "\n",
            "✅ Unzipping complete! The 'unzipped_pdfs' folder now contains 1 items.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Find All PDFs Recursively and Rename Them\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# This is the top-level folder we need to search inside.\n",
        "unzipped_pdfs_folder = \"unzipped_pdfs\"\n",
        "\n",
        "print(\"\\n--- Starting Step 4: Finding and renaming all PDFs ---\")\n",
        "\n",
        "# This list will store the full path to every PDF we find.\n",
        "all_pdf_paths = []\n",
        "\n",
        "# os.walk is a powerful function that goes through a directory and all its subdirectories.\n",
        "# This is what makes our script robust to the nested folder structure.\n",
        "for root, dirs, files in os.walk(unzipped_pdfs_folder):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "            # We save the full, correct path to each PDF.\n",
        "            all_pdf_paths.append(os.path.join(root, file))\n",
        "\n",
        "total_files_found = len(all_pdf_paths)\n",
        "if total_files_found == 0:\n",
        "    print(\"❌ ERROR: No PDF files were found even after searching all subfolders.\")\n",
        "    print(\"Please check the 'unzipped_pdfs' folder to see if the files are there.\")\n",
        "else:\n",
        "    print(f\"✅ Success! Found {total_files_found} PDF files inside the subfolders.\")\n",
        "    print(\"Proceeding to rename...\")\n",
        "\n",
        "    name_mapping = {}\n",
        "    # This list will store the NEW paths of the renamed files for the next step.\n",
        "    renamed_files_paths = []\n",
        "\n",
        "    # Now, we loop through the full paths we found and rename each file.\n",
        "    for i, old_path in enumerate(all_pdf_paths):\n",
        "        try:\n",
        "            # The new path will be in the same directory as the old one.\n",
        "            new_path = os.path.join(os.path.dirname(old_path), f\"{i:04d}.pdf\")\n",
        "\n",
        "            # We map the new simple name back to the original full path for our records.\n",
        "            name_mapping[os.path.basename(new_path)] = old_path\n",
        "            os.rename(old_path, new_path)\n",
        "            renamed_files_paths.append(new_path)\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Could not rename '{os.path.basename(old_path)}'. Error: {e}\")\n",
        "\n",
        "    # Save our mapping key for future reference.\n",
        "    with open('filename_mapping.json', 'w') as f:\n",
        "        json.dump(name_mapping, f, indent=4)\n",
        "\n",
        "    print(f\"\\n✅ Renaming complete! {len(name_mapping)} files are now in a simple format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWYeCCfiqNYG",
        "outputId": "5d547f58-5628-4565-d75a-9f7340f571e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Step 4: Finding and renaming all PDFs ---\n",
            "✅ Success! Found 900 PDF files inside the subfolders.\n",
            "Proceeding to rename...\n",
            "\n",
            "✅ Renaming complete! 900 files are now in a simple format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Extract Text Using pdfplumber\n",
        "\n",
        "import pdfplumber\n",
        "import os\n",
        "\n",
        "# These are the folders we are working with\n",
        "text_output_folder = \"extracted_text\"\n",
        "\n",
        "print(\"\\n--- Starting Step 5: Extracting text with the upgraded pdfplumber library ---\")\n",
        "\n",
        "# This script relies on the 'renamed_files_paths' list created in the previous cell.\n",
        "# This check ensures we have the list before we start.\n",
        "if 'renamed_files_paths' not in locals() or not renamed_files_paths:\n",
        "     print(\"❌ ERROR: Could not find the list of renamed files.\")\n",
        "     print(\"Please re-run the previous cell (Step 4) to generate the file list.\")\n",
        "else:\n",
        "    total_files_to_extract = len(renamed_files_paths)\n",
        "    print(f\"Found {total_files_to_extract} cleaned PDF files to process...\")\n",
        "\n",
        "    # Loop through our list of clean, renamed PDF paths\n",
        "    for i, pdf_path in enumerate(renamed_files_paths):\n",
        "        # Get the simple filename (e.g., \"0001.pdf\") for progress messages\n",
        "        pdf_filename = os.path.basename(pdf_path)\n",
        "\n",
        "        # Print a progress update every 50 files to show it's working\n",
        "        if (i + 1) % 50 == 0 or i == 0:\n",
        "            print(f\"Processing file {i+1}/{total_files_to_extract}: {pdf_filename}...\")\n",
        "\n",
        "        try:\n",
        "            full_text = \"\"\n",
        "            # Open the PDF with pdfplumber\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                # Loop through each page in the PDF\n",
        "                for page in pdf.pages:\n",
        "                    # .extract_text() is excellent at preserving the layout of tables.\n",
        "                    # x_tolerance=2 helps in aligning text that is slightly offset.\n",
        "                    text = page.extract_text(x_tolerance=2)\n",
        "                    if text:\n",
        "                        full_text += text + \"\\n\\n--- Page Break ---\\n\\n\"\n",
        "\n",
        "            # Create the output filename (e.g., \"0001.txt\")\n",
        "            text_filename = pdf_filename.replace('.pdf', '.txt')\n",
        "            output_path = os.path.join(text_output_folder, text_filename)\n",
        "\n",
        "            # Save the extracted text to the new file\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(full_text)\n",
        "        except Exception as e:\n",
        "            # If one PDF is corrupted or unreadable, this will log the error and continue\n",
        "            print(f\"   ⚠️ Could not process {pdf_filename}. It may be corrupted or empty. Error: {e}\")\n",
        "\n",
        "    print(f\"\\n✅✅✅ Data Preparation Complete! ✅✅✅\")\n",
        "    print(f\"High-quality text from all files has been extracted to the '{text_output_folder}' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLJKVtwQrC8U",
        "outputId": "f726c093-b743-48ad-c75d-9ad3929f71a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Step 5: Extracting text with the upgraded pdfplumber library ---\n",
            "Found 900 cleaned PDF files to process...\n",
            "Processing file 1/900: 0000.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 50/900: 0049.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 100/900: 0099.pdf...\n",
            "Processing file 150/900: 0149.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 200/900: 0199.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 250/900: 0249.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 300/900: 0299.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 350/900: 0349.pdf...\n",
            "Processing file 400/900: 0399.pdf...\n",
            "Processing file 450/900: 0449.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 500/900: 0499.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 550/900: 0549.pdf...\n",
            "Processing file 600/900: 0599.pdf...\n",
            "Processing file 650/900: 0649.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
            "WARNING:pdfminer.pdffont:Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 700/900: 0699.pdf...\n",
            "Processing file 750/900: 0749.pdf...\n",
            "Processing file 800/900: 0799.pdf...\n",
            "Processing file 850/900: 0849.pdf...\n",
            "Processing file 900/900: 0899.pdf...\n",
            "\n",
            "✅✅✅ Data Preparation Complete! ✅✅✅\n",
            "High-quality text from all files has been extracted to the 'extracted_text' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "l2V-L8Hf2z6D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Python library for Google's models\n",
        "!pip install -q -U google-generativeai pandas\n"
      ],
      "metadata": {
        "id": "J5aeu9bG3xcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "EwWUMu3f30tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Securely access the API key you stored in Colab's secrets.\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "    print(\"✅ Gemini API key configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Error configuring API key.\")\n",
        "    print(\"Please make sure you have created a secret named 'GEMINI_API_KEY' and given this notebook access to it.\")\n",
        "\n",
        "# Verify that the input folder from our previous work exists.\n",
        "text_input_folder = \"extracted_text\"\n",
        "if not os.path.exists(text_input_folder):\n",
        "     print(f\"❌ CRITICAL ERROR: The input folder '{text_input_folder}' was not found.\")"
      ],
      "metadata": {
        "id": "rcl2k9Fl3eGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 2 - Step 2: Define the AI Structuring Function\n",
        "\n",
        "import json\n",
        "import time\n",
        "\n",
        "def get_structured_data_from_llm(text_content, filename):\n",
        "    \"\"\"\n",
        "    Acts as our AI Analyst. It sends text to the Gemini model and asks it\n",
        "    to extract structured data based on a specific prompt and JSON schema.\n",
        "\n",
        "    Args:\n",
        "        text_content (str): The raw text extracted from a PDF.\n",
        "        filename (str): The original filename, for context.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted data, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    # This JSON schema is our instruction manual for the AI.\n",
        "    # We are telling it exactly what fields we want and what type of data they should be.\n",
        "    # This is flexible enough to handle both specific estimates and general guides.\n",
        "    json_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"project_type\": {\"type\": \"string\", \"description\": \"A general category for the project, like 'Kitchen Remodel', 'Roofing Guide', or 'Interior Painting Estimate'.\"},\n",
        "            \"summary_description\": {\"type\": \"string\", \"description\": \"A concise, one-sentence summary of the document's main purpose or the scope of work.\"},\n",
        "            \"total_cost\": {\"type\": \"number\", \"description\": \"The final, total cost or grand total of the project. If it is a general guide or no total is found, this MUST be null.\"},\n",
        "            \"line_items\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"item_description\": {\"type\": \"string\"},\n",
        "                        \"item_cost\": {\"type\": \"number\"}\n",
        "                    }\n",
        "                },\n",
        "                \"description\": \"A list of individual work items and their costs. If none are found, this should be an empty list [].\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"project_type\", \"summary_description\", \"total_cost\", \"line_items\"]\n",
        "    }\n",
        "\n",
        "    # We configure the model to always return its output in the JSON format we defined.\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=json_schema\n",
        "    )\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash-latest', generation_config=generation_config)\n",
        "\n",
        "    # This prompt is the direct instruction to our AI Analyst.\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following text from a document originally titled '{filename}'.\n",
        "    Your task is to act as a data analyst and extract key information based on the schema.\n",
        "\n",
        "    1.  Carefully determine the overall 'project_type'.\n",
        "    2.  Write a brief, neutral 'summary_description' of the document's content.\n",
        "    3.  Find the final 'total_cost'. It is CRITICAL that if no specific total project cost is listed, you return null for this field. Do not guess or sum up line items.\n",
        "    4.  Extract any specific 'line_items' that have an associated cost. If the document is a general guide with no line items, return an empty list.\n",
        "\n",
        "    Here is the text:\n",
        "    ---\n",
        "    {text_content[:20000]}\n",
        "    ---\n",
        "    \"\"\"\n",
        "\n",
        "    # This loop will retry the API call if it fails, which makes our script more robust.\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return json.loads(response.text)\n",
        "        except Exception as e:\n",
        "            print(f\"      - API Error: {e}. Retrying in {2**attempt}s...\")\n",
        "            time.sleep(2**attempt)\n",
        "\n",
        "    print(f\"      - Failed to get a valid response for {filename} after several retries.\")\n",
        "    return None\n",
        "\n",
        "print(\"✅ AI Analyst function defined successfully.\")\n",
        "print(\"Ready to process the text files in the next step.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEMh-one3pY6",
        "outputId": "a9225c1c-2d49-4f07-8293-c63020375514"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AI Analyst function defined successfully.\n",
            "Ready to process the text files in the next step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phase 2 - Step 3 (The Definitive Robust Script)\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "text_input_folder = \"extracted_text\"\n",
        "output_csv_path = \"structured_data.csv\"\n",
        "start_from_file_number = 0 # If the script stops, you can set this to the last file number to resume\n",
        "# -------------------\n",
        "\n",
        "# --- THE FIX PART 1: A more patient retry function ---\n",
        "# We are moving the retry logic into the function itself to make it more robust.\n",
        "def get_structured_data_from_llm_patiently(text_content, filename):\n",
        "    # This is the same schema and model setup as before\n",
        "    json_schema = { \"type\": \"object\", \"properties\": { \"project_type\": {\"type\": \"string\"}, \"summary_description\": {\"type\": \"string\"}, \"total_cost\": {\"type\": \"number\"}, \"line_items\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"item_description\": {\"type\": \"string\"}, \"item_cost\": {\"type\": \"number\"} } } } }, \"required\": [\"project_type\", \"summary_description\", \"total_cost\", \"line_items\"] }\n",
        "    generation_config = genai.GenerationConfig( response_mime_type=\"application/json\", response_schema=json_schema )\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash-latest', generation_config=generation_config)\n",
        "    prompt = f\"Analyze the following text from a document originally titled '{filename}'. Act as a data analyst. Extract the overall 'project_type', a brief 'summary_description', the final 'total_cost' (or null if not found), and a list of 'line_items' with costs (or an empty list). TEXT: --- {text_content[:20000]} ---\"\n",
        "\n",
        "    # We will use longer, more patient delays for retries\n",
        "    retry_delays = [5, 10, 15] # Wait 5s, then 10s, then 15s\n",
        "    for attempt, delay in enumerate(retry_delays):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return json.loads(response.text)\n",
        "        except Exception as e:\n",
        "            print(f\"      - API Error on attempt {attempt+1}: {e}. Retrying in {delay}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "    print(f\"      - FAILED to get a valid response for {filename} after all retries.\")\n",
        "    return None\n",
        "# ----------------------------------------------------\n",
        "\n",
        "\n",
        "print(f\"--- Starting the DEFINITIVE structuring process on the '{text_input_folder}' folder ---\")\n",
        "text_files = sorted([f for f in os.listdir(text_input_folder) if f.endswith(\".txt\")])\n",
        "total_files = len(text_files)\n",
        "\n",
        "if total_files > 0:\n",
        "    all_structured_data = []\n",
        "    print(f\"Found {total_files} text files. This will be very slow but very reliable...\")\n",
        "\n",
        "    # Loop through every file, starting from the number you specify\n",
        "    for i, filename in enumerate(text_files):\n",
        "        # This is the resume logic\n",
        "        if i < start_from_file_number:\n",
        "            continue\n",
        "\n",
        "        print(f\"  Processing file {i+1}/{total_files}: {filename}...\")\n",
        "        filepath = os.path.join(text_input_folder, filename)\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Call our new, more patient function\n",
        "        structured_data = get_structured_data_from_llm_patiently(content, filename)\n",
        "\n",
        "        if structured_data:\n",
        "            structured_data['source_file'] = filename\n",
        "            all_structured_data.append(structured_data)\n",
        "        else:\n",
        "            print(f\"   - Skipping {filename} as no structured data was returned.\")\n",
        "\n",
        "        # --- THE FIX PART 2: A much longer pause between files ---\n",
        "        # We will wait 10 seconds between processing each file to be extremely safe.\n",
        "        time.sleep(10)\n",
        "        # --------------------------------------------------------\n",
        "\n",
        "    if all_structured_data:\n",
        "        print(\"\\n✅ Processing complete! Compiling results into a CSV file...\")\n",
        "        df = pd.DataFrame(all_structured_data)\n",
        "        df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "        print(f\"\\n✅✅✅ Phase 2 Complete! ✅✅✅\")\n",
        "        print(f\"Successfully created '{output_csv_path}' with {len(df)} rows.\")\n",
        "        display(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obVr386v7V_x",
        "outputId": "86e4268d-021a-4540-85da-4d0068561ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting the DEFINITIVE structuring process on the 'extracted_text' folder ---\n",
            "Found 900 text files. This will be very slow but very reliable...\n",
            "  Processing file 1/900: 0000.txt...\n"
          ]
        }
      ]
    }
  ]
}